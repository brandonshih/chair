{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def write_frames(folder, input_video):\n",
    "    \"\"\"take in a folder and a video and then write all the frames into it. folder must already exist\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder : str\n",
    "            existing folder path to save frames to\n",
    "        input_video : str\n",
    "            path to saved input fideo to turn into frames\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    outfile_prefix = input_video[0 : input_video.find(\".\")] + \"_\"\n",
    "    vs = cv2.VideoCapture(os.path.join(folder, input_video))\n",
    "    while(True):\n",
    "        # read the next frame from the file\n",
    "        grabbed, frame = vs.read()\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        if not grabbed:\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            outfile_name = outfile_prefix + str(i) + \".jpg\"\n",
    "            cv2.imwrite(os.path.join(folder, outfile_name), frame)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_frames('C:\\\\Users\\\\user\\\\Desktop\\\\videos', 'empty_chair.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_v3 import (\n",
    "    decode_predictions,\n",
    "    preprocess_input,\n",
    ")\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write an annotated output video\n",
    "# #of predictions must match number of frames in the video\n",
    "OUTPUT_VIDEO_DIR = 'C:\\\\Users\\\\user\\\\chair_model'\n",
    "\n",
    "def create_output_video(\n",
    "    predictions,\n",
    "    input_vid_location,\n",
    "    out_location=OUTPUT_VIDEO_DIR,\n",
    "    frame_jump=10,\n",
    "    out_name=None,\n",
    "):\n",
    "    \"\"\"take in a video and create an output video with annotated predictions on it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : list\n",
    "        list of predictions for every frame in the video\n",
    "    input_video_location : str\n",
    "        path to saved input fideo to run on\n",
    "    out_location : str\n",
    "        path to write the annotated video to\n",
    "    frame_jump : int\n",
    "        runt/ write predictions on every nth frame, 1 would be predicting writing to every frame\n",
    "    out_name : str\n",
    "        override path and file name to write the video to\n",
    "    Returns\n",
    "    -------\n",
    "    output_video_location: str\n",
    "        where the annotated video was written to\n",
    "\n",
    "    \"\"\"\n",
    "    # initialize the video stream, pointer to output video file, and\n",
    "    # frame dimensions\n",
    "    print(\"processing video :\", input_vid_location)\n",
    "\n",
    "    if out_name:\n",
    "        outputFileName = out_name\n",
    "    else:\n",
    "        outputFileName = \"annotated_\" + os.path.basename(input_vid_location)\n",
    "    output_video_location = os.path.join(out_location, outputFileName)\n",
    "\n",
    "    vs = cv2.VideoCapture(input_vid_location)\n",
    "\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length != len(predictions):\n",
    "        print(\"frames and video length do not match. num frames = \", length)\n",
    "        return None\n",
    "\n",
    "    writer = None\n",
    "    (W, H) = (None, None)\n",
    "    # loop over frames from the video file stream\n",
    "    pred_count = 0\n",
    "    text = \"bed score prediction\"\n",
    "    while True:\n",
    "        # read the next frame from the file\n",
    "        (grabbed, frame) = vs.read()\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        # of the stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "        # if the frame dimensions are empty, grab them\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        output = frame.copy()\n",
    "        # draw the activity on the output frame\n",
    "\n",
    "        if (pred_count % frame_jump) == 0:\n",
    "            text = f\"bed score: {predictions[pred_count]}\"\n",
    "            # print(\"writing prediction \", pred_count, text)\n",
    "        pred_count += 1\n",
    "        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (0, 255, 0), 5)\n",
    "        # check if the video writer is None\n",
    "        if writer is None:\n",
    "            # initialize our video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "            writer = cv2.VideoWriter(output_video_location, fourcc, fps, (W, H), True)\n",
    "\n",
    "        # write the output frame to disk\n",
    "        writer.write(output)\n",
    "        # print(\"wrote frame\")\n",
    "\n",
    "    # release the file pointers\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    if writer:\n",
    "        writer.release()\n",
    "        out = output_video_location\n",
    "    else:\n",
    "        out = output_img_location\n",
    "    vs.release()\n",
    "    return output_video_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop test images to only contain chair\n",
    "\n",
    "CONFIDENCE = 0.5\n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.4\n",
    "\n",
    "config_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.cfg'\n",
    "weights_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.weights'\n",
    "class_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\coco.names'\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "classes = []\n",
    "with open(class_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "n = 0\n",
    "test_image_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\test\\\\test_folder'\n",
    "\n",
    "for filename in os.listdir(test_image_path)[:10]:\n",
    "    outfile_name = filename[0 : filename.find('.')] + '_crop' + '.jpg'\n",
    "\n",
    "    img = cv2.imread(os.path.join(test_image_path, filename))\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # Scaling factor used to make bounding box slightly larger to encompass entire object\n",
    "    scaling_factor = 1.2\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > CONFIDENCE:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width * scaling_factor)\n",
    "                h = int(detection[3] * height * scaling_factor)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "    #         color = colors[i]\n",
    "    #         cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "    #         cv2.putText(img, label, (x, y + 30), font, 3, color, 3)\n",
    "            if label in ['chair']:\n",
    "                crop_img = img[y:y+h, x:x+w]\n",
    "                cv2.imwrite(os.path.join(test_image_path, outfile_name), crop_img)\n",
    "                \n",
    "            \n",
    "\n",
    "# cv2.imshow(\"Image\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE = 0.5\n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5\n",
    "config_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.cfg'\n",
    "weights_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.weights'\n",
    "class_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\coco.names'\n",
    "\n",
    "labels = open(class_path).read().strip().split(\"\\n\")\n",
    "colors = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "\n",
    "path_name ='C:\\\\Users\\\\user\\\\chair_model\\\\test\\\\test_folder\\\\empty_chair_90_205.jpg'\n",
    "image = cv2.imread(path_name)\n",
    "h, w = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "layer_outputs = net.forward(ln)\n",
    "\n",
    "\n",
    "font_scale = 1\n",
    "thickness = 1\n",
    "boxes, confidences, class_ids = [], [], []\n",
    "# loop over each of the layer outputs\n",
    "for output in layer_outputs:\n",
    "    # loop over each of the object detections\n",
    "    for detection in output:\n",
    "        # extract the class id (label) and confidence (as a probability) of\n",
    "        # the current object detection\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        # discard out weak predictions by ensuring the detected\n",
    "        # probability is greater than the minimum probability\n",
    "        if confidence > CONFIDENCE:\n",
    "            # scale the bounding box coordinates back relative to the\n",
    "            # size of the image, keeping in mind that YOLO actually\n",
    "            # returns the center (x, y)-coordinates of the bounding\n",
    "            # box followed by the boxes' width and height\n",
    "            box = detection[:4] * np.array([w, h, w, h])\n",
    "            (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "            # use the center (x, y)-coordinates to derive the top and\n",
    "            # and left corner of the bounding box\n",
    "            x = int(centerX - (width / 2))\n",
    "            y = int(centerY - (height / 2))\n",
    "            # update our list of bounding box coordinates, confidences,\n",
    "            # and class IDs\n",
    "            boxes.append([x, y, int(width), int(height)])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "            \n",
    "idxs = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
    "\n",
    "if len(idxs) > 0:\n",
    "    # loop over the indexes we are keeping\n",
    "    for i in idxs.flatten():\n",
    "        # extract the bounding box coordinates\n",
    "        x, y = boxes[i][0], boxes[i][1]\n",
    "        w, h = boxes[i][2], boxes[i][3]\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = [int(c) for c in colors[class_ids[i]]]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color=color, thickness=thickness)\n",
    "        text = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "        # calculate text width & height to draw the transparent boxes as background of the text\n",
    "        (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
    "        text_offset_x = x\n",
    "        text_offset_y = y - 5\n",
    "        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
    "        overlay = image.copy()\n",
    "        cv2.rectangle(overlay, box_coords[0], box_coords[1], color=color, thickness=cv2.FILLED)\n",
    "        # add opacity (transparency to the box)\n",
    "        image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
    "        # now put the text (label: confidence %)\n",
    "        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "\n",
    "cv2.imshow('image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
