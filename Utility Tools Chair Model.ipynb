{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_v3 import (\n",
    "    decode_predictions,\n",
    "    preprocess_input,\n",
    ")\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write All Frames From Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_frames(folder, input_video):\n",
    "    \"\"\"take in a folder and a video and then write all the frames into it. folder must already exist\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder : str\n",
    "            existing folder path to save frames to\n",
    "        input_video : str\n",
    "            path to saved input fideo to turn into frames\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    outfile_prefix = input_video[0 : input_video.find(\".\")] + \"_\"\n",
    "    vs = cv2.VideoCapture(os.path.join(folder, input_video))\n",
    "    while(True):\n",
    "        # read the next frame from the file\n",
    "        grabbed, frame = vs.read()\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        if not grabbed:\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            outfile_name = outfile_prefix + str(i) + \".jpg\"\n",
    "            cv2.imwrite(os.path.join(folder, outfile_name), frame)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\user\\\\chair_model\\\\vids'\n",
    "for filenames in os.listdir(path):\n",
    "    write_frames(path, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_frames('C:\\\\Users\\\\user\\\\Desktop\\\\videos', 'empty_chair.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take in video and create output video with annotated predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write an annotated output video\n",
    "# #of predictions must match number of frames in the video\n",
    "OUTPUT_VIDEO_DIR = 'C:\\\\Users\\\\user\\\\chair_model'\n",
    "\n",
    "def create_output_video(\n",
    "    predictions,\n",
    "    input_vid_location,\n",
    "    out_location=OUTPUT_VIDEO_DIR,\n",
    "    frame_jump=10,\n",
    "    out_name=None,\n",
    "):\n",
    "    \"\"\"take in a video and create an output video with annotated predictions on it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : list\n",
    "        list of predictions for every frame in the video\n",
    "    input_video_location : str\n",
    "        path to saved input fideo to run on\n",
    "    out_location : str\n",
    "        path to write the annotated video to\n",
    "    frame_jump : int\n",
    "        runt/ write predictions on every nth frame, 1 would be predicting writing to every frame\n",
    "    out_name : str\n",
    "        override path and file name to write the video to\n",
    "    Returns\n",
    "    -------\n",
    "    output_video_location: str\n",
    "        where the annotated video was written to\n",
    "\n",
    "    \"\"\"\n",
    "    # initialize the video stream, pointer to output video file, and\n",
    "    # frame dimensions\n",
    "    print(\"processing video :\", input_vid_location)\n",
    "\n",
    "    if out_name:\n",
    "        outputFileName = out_name\n",
    "    else:\n",
    "        outputFileName = \"annotated_\" + os.path.basename(input_vid_location)\n",
    "    output_video_location = os.path.join(out_location, outputFileName)\n",
    "\n",
    "    vs = cv2.VideoCapture(input_vid_location)\n",
    "\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length != len(predictions):\n",
    "        print(\"frames and video length do not match. num frames = \", length)\n",
    "        return None\n",
    "\n",
    "    writer = None\n",
    "    (W, H) = (None, None)\n",
    "    # loop over frames from the video file stream\n",
    "    pred_count = 0\n",
    "    text = \"bed score prediction\"\n",
    "    while True:\n",
    "        # read the next frame from the file\n",
    "        (grabbed, frame) = vs.read()\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        # of the stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "        # if the frame dimensions are empty, grab them\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        output = frame.copy()\n",
    "        # draw the activity on the output frame\n",
    "\n",
    "        if (pred_count % frame_jump) == 0:\n",
    "            text = f\"bed score: {predictions[pred_count]}\"\n",
    "            # print(\"writing prediction \", pred_count, text)\n",
    "        pred_count += 1\n",
    "        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (0, 255, 0), 5)\n",
    "        # check if the video writer is None\n",
    "        if writer is None:\n",
    "            # initialize our video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "            writer = cv2.VideoWriter(output_video_location, fourcc, fps, (W, H), True)\n",
    "\n",
    "        # write the output frame to disk\n",
    "        writer.write(output)\n",
    "        # print(\"wrote frame\")\n",
    "\n",
    "    # release the file pointers\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    if writer:\n",
    "        writer.release()\n",
    "        out = output_video_location\n",
    "    else:\n",
    "        out = output_img_location\n",
    "    vs.release()\n",
    "    return output_video_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Images Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Crop Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop test images to only contain chair\n",
    "\n",
    "CONFIDENCE = 0.5\n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.4\n",
    "\n",
    "config_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.cfg'\n",
    "weights_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.weights'\n",
    "class_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\coco.names'\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "classes = []\n",
    "with open(class_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "n = 0\n",
    "test_image_path = 'E:\\\\video_unique\\\\empty'\n",
    "\n",
    "for filename in os.listdir(test_image_path)[:10]:\n",
    "    outfile_name = filename[0 : filename.find('.')] + '_crop' + '.jpg'\n",
    "\n",
    "    img = cv2.imread(os.path.join(test_image_path, filename))\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # Scaling factor used to make bounding box slightly larger to encompass entire object\n",
    "    scaling_factor = 1.5\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > CONFIDENCE:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width * scaling_factor)\n",
    "                h = int(detection[3] * height * scaling_factor)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            \n",
    "            if label in ['chair']:\n",
    "                crop_img = img[y:y+h, x:x+w]\n",
    "                cv2.imwrite(os.path.join(test_image_path, outfile_name), crop_img)\n",
    "                \n",
    "            \n",
    "\n",
    "# cv2.imshow(\"Image\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop test images to only contain chair\n",
    "\n",
    "CONFIDENCE = 0.5\n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.4\n",
    "\n",
    "config_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.cfg'\n",
    "weights_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\yolov3.weights'\n",
    "class_path = 'C:\\\\Users\\\\user\\\\chair_model\\\\coco.names'\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "classes = []\n",
    "with open(class_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = 'E:\\\\video_unique\\\\empty'\n",
    "\n",
    "for filename in os.listdir(test_image_path)[:1]:\n",
    "    outfile_name = filename[0 : filename.find('.')] + '_crop' + '.jpg'\n",
    "\n",
    "    img = cv2.imread(os.path.join(test_image_path, filename))\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    class_ids, confidences, boxes = [], [], []\n",
    "    \n",
    "    scaling_factor = 1.5\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > CONFIDENCE:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width * scaling_factor)\n",
    "                h = int(detection[3] * height * scaling_factor)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label, (x, y + 30), font, 3, color, 3)\n",
    "                            \n",
    "#             if label in ['chair']:\n",
    "#                 crop_img = img[y:y+h, x:x+w]\n",
    "#                 cv2.imwrite(os.path.join(test_image_path, outfile_name), crop_img)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFMPEG Identify Unique Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 9.2.1 (GCC) 20200122\n",
      "  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libmfx --enable-amf --enable-ffnvcodec --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 000001875087a280] st: 1 edit list: 1 Missing key frame while searching for timestamp: 0\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 000001875087a280] st: 1 edit list 1 Cannot find an index entry before timestamp: 0.\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'E:\\\\vids\\\\in_pillow.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41avc1\n",
      "    creation_time   : 2020-05-17T00:05:13.000000Z\n",
      "    encoder         : vlc 3.0.10 stream output\n",
      "    encoder-eng     : vlc 3.0.10 stream output\n",
      "  Duration: 00:01:25.12, start: 0.000000, bitrate: 3537 kb/s\n",
      "    Stream #0:0(eng): Audio: mp3 (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2020-05-17T00:05:13.000000Z\n",
      "      handler_name    : SoundHandler\n",
      "    Stream #0:1(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 3402 kb/s, 30 fps, 30 tbr, 1000k tbn, 60.01 tbc (default)\n",
      "    Metadata:\n",
      "      rotate          : 180\n",
      "      creation_time   : 2020-05-17T00:05:13.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "    Side data:\n",
      "      displaymatrix: rotation of -180.00 degrees\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (h264 (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0000018755abc280] deprecated pixel format used, make sure you did set range correctly\n",
      "Output #0, image2, to 'E:\\\\presort_images\\\\in_pillow_frame%03d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41avc1\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0(eng): Video: mjpeg, yuvj420p(pc), 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 30 fps, 30 tbn, 30 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 mjpeg\n",
      "      creation_time   : 2020-05-17T00:05:13.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "      displaymatrix: rotation of -0.00 degrees\n",
      "frame=    2 fps=0.0 q=2.0 size=N/A time=00:00:00.03 bitrate=N/A speed=0.0664x    \n",
      "frame=    3 fps=3.0 q=2.0 size=N/A time=00:00:08.36 bitrate=N/A speed=8.35x    \n",
      "frame=    5 fps=3.3 q=2.0 size=N/A time=00:00:25.03 bitrate=N/A speed=16.6x    \n",
      "frame=    6 fps=3.0 q=2.0 size=N/A time=00:00:33.36 bitrate=N/A speed=16.6x    \n",
      "frame=    8 fps=3.2 q=2.0 size=N/A time=00:00:50.03 bitrate=N/A speed=19.9x    \n",
      "frame=    9 fps=3.0 q=2.0 size=N/A time=00:00:58.36 bitrate=N/A speed=19.4x    \n",
      "frame=   13 fps=3.7 q=2.0 size=N/A time=00:01:22.23 bitrate=N/A speed=23.4x    \n",
      "frame=   13 fps=3.6 q=2.0 Lsize=N/A time=00:01:22.76 bitrate=N/A speed=  23x    \n",
      "video:2500kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
     ]
    }
   ],
   "source": [
    "# FFMPEG tool used to identify unique frames from video samples\n",
    "\n",
    "!ffmpeg -i E:\\\\vids\\\\in_pillow.mp4 -q:v 2 -vf select=\"eq(pict_type\\,PICT_TYPE_I)\" \\\n",
    "        -vsync 0 E:\\\\presort_images\\\\in_pillow_frame%03d.jpg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
