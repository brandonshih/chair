{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "WARNING:tensorflow:From <ipython-input-2-3e1b5a674b1d>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import datetime, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from tensorboard import notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils.layer_utils import count_params\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image Augmentation and Training Parameters\n",
    "\n",
    "class Config:\n",
    "    # Data Augmentation Parameters\n",
    "    DATA_AUG_ON = True\n",
    "    DATA_AUG_ROTATE = 20\n",
    "    DATA_AUG_FLIP = True\n",
    "    DATA_AUG_WIDTH_SHIFT = 0.2\n",
    "    DATA_AUG_HEIGHT_SHIFT = 0.2\n",
    "    DATA_AUG_ZOOM = 0.2\n",
    "    DATA_AUG_SHEAR = 0.2\n",
    "    # Epoch to start learning rate decay\n",
    "    MODEL_TRAIN_DECAY = True\n",
    "    MODEL_START_DECAY = 0\n",
    "    # Exponential Learning Rate Decay\n",
    "    MODEL_DECAY_EXPONENTIAL = True\n",
    "    MODEL_PERCENT_DECAY = 0.1\n",
    "    # Sigmoidal Learning Rate Decay\n",
    "    MODEL_DECAY_SIGMOID = False\n",
    "    # Drop-Based Learning Rate Decay\n",
    "    MODEL_DECAY_DROP = False\n",
    "    MODEL_DROP_RATE = 0.5\n",
    "    MODEL_EPOCH_DROP = 10\n",
    "    # General Model Parameters\n",
    "    MODEL_TRAIN_EPOCHS = [30]\n",
    "    MODEL_TRAIN_LEARN_RATE = [0.005]\n",
    "    MODEL_TRAIN_BATCH_SIZE = 32\n",
    "    MODEL_TRAIN_FIRST_TRAINABLE_LAYER = 250\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 21,942,177\n",
      "Trainable Parameters: 10,676,737\n",
      "Non-Trainable Parameters: 11,265,440\n"
     ]
    }
   ],
   "source": [
    "# Define Base Model\n",
    "\n",
    "base_model = InceptionV3(weights = 'imagenet', include_top = False)\n",
    "\n",
    "MODEL_TRAINABLE_BASE = True\n",
    "\n",
    "if MODEL_TRAINABLE_BASE:\n",
    "    base_model.trainable = True\n",
    "    if config.MODEL_TRAIN_FIRST_TRAINABLE_LAYER:\n",
    "        for layer in base_model.layers[:config.MODEL_TRAIN_FIRST_TRAINABLE_LAYER]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[config.MODEL_TRAIN_FIRST_TRAINABLE_LAYER:]:\n",
    "            layer.trainable = True\n",
    "else:\n",
    "    base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(5e-3),\n",
    "                bias_regularizer = regularizers.l2(5e-3))(x)\n",
    "x = Dense(1, activation = 'sigmoid', name = 'output')(x)\n",
    "\n",
    "model = Model(inputs = base_model.input, outputs = x)\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "trainable_count = count_params(model.trainable_weights)\n",
    "non_trainable_count = count_params(model.non_trainable_weights)\n",
    "print('Total Parameters: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable Parameters: {:,}'.format(trainable_count))\n",
    "print('Non-Trainable Parameters: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2197 images belonging to 2 classes.\n",
      "Found 37 images belonging to 2 classes.\n",
      "Found 2700 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'emptyChair': 0, 'inChair': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.DATA_AUG_ON:\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                       rotation_range = config.DATA_AUG_ROTATE,\n",
    "                                       horizontal_flip = config.DATA_AUG_FLIP,\n",
    "                                       width_shift_range = config.DATA_AUG_WIDTH_SHIFT,\n",
    "                                       height_shift_range = config.DATA_AUG_HEIGHT_SHIFT,\n",
    "                                       zoom_range = config.DATA_AUG_ZOOM,\n",
    "                                       shear_range = config.DATA_AUG_SHEAR)\n",
    "\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "    \n",
    "valid_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "    \n",
    "train_generator = train_datagen.flow_from_directory('C:\\\\Users\\\\user\\\\chair_model\\\\training',\n",
    "                                              target_size = (299, 299),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              classes = ['emptyChair', 'inChair'],\n",
    "                                              batch_size = config.MODEL_TRAIN_BATCH_SIZE,\n",
    "                                              seed = 724,\n",
    "                                              class_mode = 'binary',\n",
    "                                              shuffle = False)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory('C:\\\\Users\\\\user\\\\chair_model\\\\valid',\n",
    "                                              target_size = (299, 299),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              classes = ['emptyChair', 'inChair'],\n",
    "                                              batch_size = config.MODEL_TRAIN_BATCH_SIZE,\n",
    "                                              seed = 724,\n",
    "                                              class_mode = 'binary',\n",
    "                                              shuffle = False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory('C:\\\\Users\\\\user\\\\chair_model\\\\test',\n",
    "                                              target_size = (299, 299),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              batch_size = 1,\n",
    "                                              seed = 724,\n",
    "                                              class_mode = None,\n",
    "                                              shuffle = False)\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = validation_generator.n//validation_generator.batch_size\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(lr = config.MODEL_TRAIN_LEARN_RATE[0]),\n",
    "              loss = losses.BinaryCrossentropy(),\n",
    "              metrics = ['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning Rate Decay Graph\n",
    "decay = []\n",
    "def scheduler(epoch):\n",
    "    learn_rate = config.MODEL_TRAIN_LEARN_RATE[0]\n",
    "    total_epochs = config.MODEL_TRAIN_EPOCHS[0]\n",
    "    \n",
    "    if config.MODEL_TRAIN_DECAY:\n",
    "        \n",
    "        if epoch < config.MODEL_START_DECAY:\n",
    "            value = learn_rate\n",
    "            decay.append(value)\n",
    "            return value\n",
    "        \n",
    "        else:\n",
    "            if config.MODEL_DECAY_EXPONENTIAL: \n",
    "                value = learn_rate * tf.math.exp(\n",
    "                        config.MODEL_PERCENT_DECAY * (config.MODEL_START_DECAY - epoch))\n",
    "                \n",
    "            if config.MODEL_DECAY_SIGMOID:\n",
    "                decay_rate = learn_rate / total_epochs\n",
    "                value = learn_rate * 1/(1 + decay_rate * epoch)\n",
    "                \n",
    "            if config.MODEL_DECAY_DROP:\n",
    "                value = learn_rate * np.power(\n",
    "                        config.MODEL_DROP_RATE, np.floor((1+epoch)/ config.MODEL_EPOCH_DROP))\n",
    "                \n",
    "            decay.append(value)\n",
    "            return value\n",
    "        \n",
    "    else:\n",
    "        return config.MODEL_TRAIN_LEARN_RATE[0]\n",
    "\n",
    "for x in np.arange(0, config.MODEL_TRAIN_EPOCHS[0] + 1):\n",
    "    scheduler(x)\n",
    "    \n",
    "plt.plot(np.linspace(0, config.MODEL_TRAIN_EPOCHS[0], config.MODEL_TRAIN_EPOCHS[0] + 1), decay)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Decay')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 68 steps, validate for 1 steps\n",
      "Epoch 1/30\n",
      "68/68 [==============================] - 72s 1s/step - loss: 0.3014 - binary_accuracy: 0.9284 - val_loss: 0.7060 - val_binary_accuracy: 0.7500\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 69s 1s/step - loss: 0.2427 - binary_accuracy: 0.9441 - val_loss: 1.1379 - val_binary_accuracy: 0.4375\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 70s 1s/step - loss: 0.2048 - binary_accuracy: 0.9584 - val_loss: 0.9950 - val_binary_accuracy: 0.5312\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 70s 1s/step - loss: 0.1418 - binary_accuracy: 0.9774 - val_loss: 1.2380 - val_binary_accuracy: 0.5312\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 70s 1s/step - loss: 0.1298 - binary_accuracy: 0.9806 - val_loss: 0.5997 - val_binary_accuracy: 0.7500\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 71s 1s/step - loss: 0.1156 - binary_accuracy: 0.9829 - val_loss: 0.5973 - val_binary_accuracy: 0.7812\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 70s 1s/step - loss: 0.1165 - binary_accuracy: 0.9880 - val_loss: 0.8051 - val_binary_accuracy: 0.6875\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 69s 1s/step - loss: 0.1433 - binary_accuracy: 0.9852 - val_loss: 1.5306 - val_binary_accuracy: 0.5938\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 69s 1s/step - loss: 0.3488 - binary_accuracy: 0.9118 - val_loss: 5.9977 - val_binary_accuracy: 0.7812\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 69s 1s/step - loss: 0.2245 - binary_accuracy: 0.9533 - val_loss: 1.1109 - val_binary_accuracy: 0.6250\n",
      "Epoch 11/30\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.1195 - binary_accuracy: 0.9920\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "68/68 [==============================] - 69s 1s/step - loss: 0.1186 - binary_accuracy: 0.9921 - val_loss: 1.6159 - val_binary_accuracy: 0.5938\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0744 - binary_accuracy: 0.9954 - val_loss: 0.6981 - val_binary_accuracy: 0.6875\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0558 - binary_accuracy: 0.9982 - val_loss: 0.7049 - val_binary_accuracy: 0.6562\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0505 - binary_accuracy: 0.9977 - val_loss: 0.5658 - val_binary_accuracy: 0.7188\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0439 - binary_accuracy: 0.9995 - val_loss: 0.6544 - val_binary_accuracy: 0.6875\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 68s 999ms/step - loss: 0.0392 - binary_accuracy: 0.9991 - val_loss: 0.6151 - val_binary_accuracy: 0.6875\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0369 - binary_accuracy: 0.9986 - val_loss: 0.5710 - val_binary_accuracy: 0.7188\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0363 - binary_accuracy: 0.9991 - val_loss: 0.5586 - val_binary_accuracy: 0.7500\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0360 - binary_accuracy: 0.9995 - val_loss: 0.5782 - val_binary_accuracy: 0.6875\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 68s 1000ms/step - loss: 0.0312 - binary_accuracy: 0.9995 - val_loss: 0.6943 - val_binary_accuracy: 0.5938\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0320 - binary_accuracy: 0.9991 - val_loss: 0.6593 - val_binary_accuracy: 0.6875\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0402 - binary_accuracy: 0.9972 - val_loss: 0.7278 - val_binary_accuracy: 0.5938\n",
      "Epoch 23/30\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.0312 - binary_accuracy: 0.9986\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "68/68 [==============================] - 68s 999ms/step - loss: 0.0312 - binary_accuracy: 0.9986 - val_loss: 0.6310 - val_binary_accuracy: 0.6250\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0287 - binary_accuracy: 0.9995 - val_loss: 0.5585 - val_binary_accuracy: 0.6562\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 68s 1000ms/step - loss: 0.0242 - binary_accuracy: 1.0000 - val_loss: 0.5650 - val_binary_accuracy: 0.6250\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 68s 999ms/step - loss: 0.0233 - binary_accuracy: 1.0000 - val_loss: 0.5575 - val_binary_accuracy: 0.6875\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0230 - binary_accuracy: 1.0000 - val_loss: 0.5080 - val_binary_accuracy: 0.7188\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0211 - binary_accuracy: 1.0000 - val_loss: 0.5367 - val_binary_accuracy: 0.6562\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0222 - binary_accuracy: 1.0000 - val_loss: 0.4534 - val_binary_accuracy: 0.7500\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 68s 1s/step - loss: 0.0199 - binary_accuracy: 1.0000 - val_loss: 0.4653 - val_binary_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "# DIFFERENT CALLBACK FUNCTIONS\n",
    "\n",
    "# Display Learning Rate Each Epoch\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print('\\nEpoch Learning Rate: ', K.eval(lr_with_decay))\n",
    "        \n",
    "# Reduced learning rate when metric has stopped improving\n",
    "reduced_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5,\n",
    "                               patience = 5, min_lr = 0.0001, verbose = 1)\n",
    "\n",
    "# Early stopping when monitored quantity has stopped improving\n",
    "early_stop = EarlyStopping()\n",
    "\n",
    "# Tensorboard Callback\n",
    "logdir = os.path.join('logs', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq = 1)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(scheduler, verbose = 0)\n",
    "call_learning_rate = MyCallback()\n",
    "\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                              steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "                              validation_data = validation_generator,\n",
    "                              validation_steps = STEP_SIZE_VALID,\n",
    "                              epochs = config.MODEL_TRAIN_EPOCHS[0],\n",
    "                              callbacks = [reduced_lr, tensorboard_callback],\n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14700), started 0:14:40 ago. (Use '!kill 14700' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2fb13bdb78f1c548\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2fb13bdb78f1c548\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs (started 0:13:50 ago; pid 14700)\n",
      "  - port 8088: logdir data/ (started 0:11:02 ago; pid 6700)\n",
      "  - port 8088: logdir logs (started 0:01:21 ago; pid 8716)\n"
     ]
    }
   ],
   "source": [
    "notebook.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.plot(history.history['binary_accuracy'], label = 'Training Accuracy')\n",
    "plt.plot(history.history['val_binary_accuracy'], label = 'Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('C:\\\\Users\\\\user\\\\saved_models\\\\basic_alldata_0.0001_25_250_1.h5')\n",
    "# model.evaluate_generator(generator = validation_generator, steps = STEP_SIZE_VALID, verbose = 1)\n",
    "\n",
    "test_generator.reset()\n",
    "pred = model.predict_generator(test_generator,\n",
    "                               steps = STEP_SIZE_TEST,\n",
    "                               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for filename in os.listdir('C:\\\\Users\\\\user\\\\chair_model\\\\test\\\\test_folder'):\n",
    "    if 'empty' in filename:\n",
    "        test_labels.append(0)\n",
    "    else:\n",
    "        test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "for th in np.linspace(0, 1, 100):\n",
    "    pred_labels = (pred > th)\n",
    "    x = accuracy_score(pred_labels, test_labels)\n",
    "    accuracy.append(x)\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.plot(np.linspace(0, 1, 100), accuracy)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Binary Classification Accuracy at Various Thresholds')\n",
    "\n",
    "plt.figure(2)\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, pred)\n",
    "average_precision = average_precision_score(test_labels, pred)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('2-class Precision-Recall Curve: AP = ''{0:0.2f}'.format(average_precision))\n",
    "\n",
    "fpr_keras, tpr_keras, thersholds_keras = roc_curve(test_labels, pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label = 'Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "pred_labels = (pred > threshold)\n",
    "\n",
    "cf_matrix = confusion_matrix(test_labels, pred_labels) \n",
    "  \n",
    "print('Confusion Matrix :')\n",
    "print(cf_matrix) \n",
    "print('Accuracy Score :', accuracy_score(test_labels, pred_labels))\n",
    "print('Report : ')\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "plt.figure(1)\n",
    "group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot = labels, fmt='', cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt('results.csv', pred_labels, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELFILENAME = 'basic_alldata_0.0001_25_250_1.h5'\n",
    "MODELPATH = 'C:\\\\Users\\\\user\\\\saved_models'\n",
    "model.save(os.path.join(MODELPATH, MODELFILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write an annotated output video\n",
    "# number of predictions must match number of frames in the video\n",
    "OUTPUT_VIDEO_DIR = 'C:\\\\Users\\\\user\\\\chair_model'\n",
    "\n",
    "def create_output_video(\n",
    "    predictions,\n",
    "    input_vid_location,\n",
    "    out_location=OUTPUT_VIDEO_DIR,\n",
    "    frame_jump = 1,\n",
    "    out_name = None,\n",
    "):\n",
    "    \"\"\"take in a video and create an output video with annotated predictions on it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : list\n",
    "        list of predictions for every frame in the video\n",
    "    input_video_location : str\n",
    "        path to saved input fideo to run on\n",
    "    out_location : str\n",
    "        path to write the annotated video to\n",
    "    frame_jump : int\n",
    "        runt/ write predictions on every nth frame, 1 would be predicting writing to every frame\n",
    "    out_name : str\n",
    "        override path and file name to write the video to\n",
    "    Returns\n",
    "    -------\n",
    "    output_video_location: str\n",
    "        where the annotated video was written to\n",
    "\n",
    "    \"\"\"\n",
    "    # initialize the video stream, pointer to output video file, and\n",
    "    # frame dimensions\n",
    "    print(\"processing video :\", input_vid_location)\n",
    "\n",
    "    if out_name:\n",
    "        outputFileName = out_name\n",
    "    else:\n",
    "        outputFileName = \"annotated_\" + os.path.basename(input_vid_location)\n",
    "    output_video_location = os.path.join(out_location, outputFileName)\n",
    "\n",
    "    vs = cv2.VideoCapture(input_vid_location)\n",
    "\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length != len(predictions):\n",
    "        print(\"frames and video length do not match. num frames = \", length)\n",
    "        return None\n",
    "\n",
    "    writer = None\n",
    "    (W, H) = (None, None)\n",
    "    # loop over frames from the video file stream\n",
    "    pred_count = 0\n",
    "    text = \"bed score prediction\"\n",
    "    while True:\n",
    "        # read the next frame from the file\n",
    "        (grabbed, frame) = vs.read()\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        # of the stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "        # if the frame dimensions are empty, grab them\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        output = frame.copy()\n",
    "        # draw the activity on the output frame\n",
    "\n",
    "        if (pred_count % frame_jump) == 0:\n",
    "            text = f\"Chair Score: {predictions[pred_count]}\"\n",
    "            # print(\"writing prediction \", pred_count, text)\n",
    "        pred_count += 1\n",
    "        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (0, 255, 0), 5)\n",
    "        # check if the video writer is None\n",
    "        if writer is None:\n",
    "            # initialize our video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "            writer = cv2.VideoWriter(output_video_location, fourcc, fps, (W, H), True)\n",
    "\n",
    "        # write the output frame to disk\n",
    "        writer.write(output)\n",
    "        # print(\"wrote frame\")\n",
    "\n",
    "    # release the file pointers\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    if writer:\n",
    "        writer.release()\n",
    "        out = output_video_location\n",
    "    else:\n",
    "        out = output_img_location\n",
    "    vs.release()\n",
    "    return output_video_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_output_video(pred_labels[:1350], \n",
    "                    'C:\\\\Users\\\\user\\\\chair_model\\\\test_videos\\\\empty_chair_90.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFMPEG tool used to identify unique frames from video samples\n",
    "\n",
    "# !ffmpeg -i \"C:\\\\Users\\\\user\\\\chair_model\\\\videos\\\\empty_chair.mp4\" -vf \"select=gt(scene\\,0.003),setpts=N/(30*TB)\" \"C:\\\\Users\\\\user\\\\chair_model\\\\videos\\\\test.mp4\" \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
